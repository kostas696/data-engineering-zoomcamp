# Crypto Streaming Pipeline: Real-Time Crypto Price Dashboard

## âœ¨ Project Overview

This project is the final course deliverable for the Data Engineering Zoomcamp. It demonstrates an **end-to-end streaming data pipeline** built on **Google Cloud Platform (GCP)** using a real-time data source (OKX WebSocket API), Apache Kafka, Spark, BigQuery, and Looker Studio.

The primary goal is to process, validate, and visualize live cryptocurrency pricing data through a fully orchestrated pipeline.

---

## ğŸ’¡ Problem Statement

Create a streaming pipeline that:
- Ingests crypto prices from a real-time API.
- Writes the data to a raw storage layer (GCS).
- Transforms and cleans the data using Spark.
- Loads the results into a partitioned BigQuery table.
- Validates the processed data.
- Visualizes the data with a dynamic Looker Studio dashboard.

---

## ğŸŒ Cloud Setup

- **Cloud Provider**: Google Cloud Platform (GCP)
- **IaC**: Terraform for provisioning (VM, IAM, service account)
- **VM**: Self-hosted Airflow + Spark running on GCE

---

## Architecture

![Architecture](images\crypto_pipeline_architecture.png)

---

## â™»ï¸ Streaming Pipeline Architecture

```
OKX WebSocket API
        â¬‡
  Dockerized Kafka Producer
        â¬‡
        Kafka
        â¬‡
  Dockerized Kafka Consumer
        â¬‡
      GCS (raw)
        â¬‡
     Spark Job (on GCE)
        â¬‡
    BigQuery Table (partitioned)
        â¬‡
  Data Validation (Python script)
        â¬‡
     Looker Studio Dashboard
```

---

## ğŸ“† Technologies Used

- **Streaming**: WebSocket, Apache Kafka (via Docker Compose)
- **Ingestion**: Kafka Consumer â” GCS
- **Processing**: Apache Spark (PySpark)
- **Orchestration**: Apache Airflow
- **Storage**: Google Cloud Storage (GCS)
- **Data Warehouse**: Google BigQuery (partitioned by ingestion time)
- **Dashboard**: Looker Studio
- **IaC**: Terraform
- **Containerization**: Docker

---

## ğŸ’¼ Project Structure

```
ğŸ“¦project-coincap-streaming-pipeline
 â”£ ğŸ“‚.env
 â”£ ğŸ“‚.venv
 â”£ ğŸ“‚dags
 â”ƒ â”£ ğŸ“œ.env
 â”ƒ â”£ ğŸ“œDockerfile
 â”ƒ â”— ğŸ“œstreaming_pipeline_dag.py
 â”£ ğŸ“‚data_ingestion
 â”ƒ â”£ ğŸ“œassets.txt
 â”ƒ â”£ ğŸ“œDockerfile
 â”ƒ â”£ ğŸ“œokx_ws_producer.py
 â”ƒ â”— ğŸ“œrequirements.txt
 â”£ ğŸ“‚gcloud
 â”ƒ â”— ğŸ“œkafka-consumer-key.json
 â”£ ğŸ“‚gcs
 â”ƒ â”£ ğŸ“œDockerfile
 â”ƒ â”£ ğŸ“œkafka_consumer_to_gcs.py
 â”ƒ â”— ğŸ“œrequirements.txt
 â”£ ğŸ“‚images
 â”ƒ â”£ ğŸ“œairflow.png
 â”ƒ â”£ ğŸ“œbigquery.png
 â”ƒ â”£ ğŸ“œcrypto_pipeline_architecture.png
 â”ƒ â”£ ğŸ“œgcs_bucket.png
 â”ƒ â”£ ğŸ“œlooker_studio.png
 â”ƒ â”— ğŸ“œrunning_consumer.png
 â”£ ğŸ“‚reports
 â”ƒ â”— ğŸ“œLooker_Studio_Reporting_-_4_14_25,_3_48â€¯PM.pdf
 â”£ ğŸ“‚spark_jobs
 â”ƒ â”— ğŸ“œspark_gcs_to_bq.py
 â”£ ğŸ“‚terraform
 â”ƒ â”£ ğŸ“‚.terraform
 â”ƒ â”£ ğŸ“‚modules
 â”ƒ â”ƒ â”£ ğŸ“‚gcs
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œmain.tf
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œoutputs.tf
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œvariables.tf
 â”ƒ â”ƒ â”£ ğŸ“‚network
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œmain.tf
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œoutputs.tf
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œvariables.tf
 â”ƒ â”ƒ â”— ğŸ“‚vm
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œmain.tf
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œoutputs.tf
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œstartup.sh
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œvariables.tf
 â”ƒ â”£ ğŸ“œ.terraform.lock.hcl
 â”ƒ â”£ ğŸ“œenable_apis.tf
 â”ƒ â”£ ğŸ“œmain.tf
 â”ƒ â”£ ğŸ“œoutputs.tf
 â”ƒ â”£ ğŸ“œproviders.tf
 â”ƒ â”£ ğŸ“œterraform.tfstate
 â”ƒ â”£ ğŸ“œterraform.tfstate.backup
 â”ƒ â”£ ğŸ“œterraform.tfvars
 â”ƒ â”— ğŸ“œvariables.tf
 â”£ ğŸ“‚tests
 â”ƒ â”— ğŸ“œsample.json
 â”£ ğŸ“‚validation
 â”ƒ â”— ğŸ“œvalidate_data.py
 â”£ ğŸ“œ.gitignore
 â”£ ğŸ“œdocker-compose.yml
 â”£ ğŸ“œMakefile
 â”£ ğŸ“œREADME.md
 â”£ ğŸ“œrequirements.txt
 â”— ğŸ“œwait-for-kafka.sh
```

---

## âš¡ Pipeline Execution

### ğŸš€ Using Makefile

```bash
# Start Docker containers and producer/consumer
make run-producer
make run-consumer

# Submit Spark transformation job manually
make submit-spark-job

# Validate latest day's data
make validate

# Run full workflow (useful for testing end-to-end)
make run-all
```

### ğŸŒ¬ï¸ Trigger via Airflow

1. Run the Airflow webserver:
```bash
airflow webserver
```

2. Run the scheduler:
```bash
airflow scheduler
```

3. Open the Airflow UI at `http://<vm_ip>:8080` and trigger the `crypto_streaming_pipeline` DAG.

---

## ğŸš§ Terraform Infrastructure Setup

```bash
cd terraform
terraform init
terraform apply -auto-approve
```

This provisions:
- GCE VM for Airflow and Spark
- Service accounts and IAM roles
- GCS bucket

---

## ğŸ”¹ BigQuery Table: `crypto_streaming.prices`

- Partitioned by `DATE(time)`
- Schema:
  - `symbol` (STRING)
  - `priceUsd` (FLOAT)
  - `time` (TIMESTAMP)

---

## ğŸ“Š Dashboard in Looker Studio

Dashboard contains:
- **Bar Chart**: Number of records on cloud
- **Bar Chart**: Average price by crypto symbol
- **Time Series Line Chart**: Price evolution over time

Title: `Real-Time Crypto Streaming Dashboard`

> Link to the dashboard: [reports\Looker_Studio_Reporting_-_4_14_25,_3_48â€¯PM.pdf]

---

## ğŸ” Future Improvements

- Add DBT for transformation versioning
- Setup CI/CD using GitHub Actions for DAG and script deployment

---

## ğŸš€ Author

**Konstantinos Soufleros**
- GitHub: [@kostas696](https://github.com/kostas696)
- Email: soufleros.kostas@gmail.com

---

> This project is part of the Data Engineering Zoomcamp course by DataTalksClub.

